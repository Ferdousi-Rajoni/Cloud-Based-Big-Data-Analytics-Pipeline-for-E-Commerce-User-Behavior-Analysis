{
	"jobConfig": {
		"name": "retailrocket_data_cleaning_job",
		"description": "",
		"role": "arn:aws:iam::727618352148:role/AWSGlueRoleForEcommerceProject",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "retailrocket_data_cleaning_job.py",
		"scriptLocation": "s3://ecommerce-user-behavior-2025/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-07-23T18:45:12.752Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://ecommerce-user-behavior-2025/temp/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-727618352148-ca-central-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom datetime import datetime\r\nfrom pyspark.sql import functions as F\r\nfrom pyspark.sql.types import TimestampType\r\nfrom pyspark.sql.window import Window\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\n\r\n# -----------------------------------------\r\n# Glue Boilerplate\r\n# -----------------------------------------\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# -----------------------------------------\r\n# Step 1: Load Raw Data\r\n# -----------------------------------------\r\nraw_df = spark.read.option(\"header\", \"true\") \\\r\n                   .option(\"inferSchema\", \"true\") \\\r\n                   .csv(\"s3://ecommerce-user-behavior-2025/raw-data/retailrocket/events.csv\")\r\n\r\nprint(f\"Initial row count: {raw_df.count()}\")\r\n\r\n# -----------------------------------------\r\n# Step 2: Remove Critical Nulls\r\n# -----------------------------------------\r\ncleaned_df = raw_df.dropna(subset=[\"visitorid\", \"event\", \"itemid\", \"timestamp\"])\r\nprint(f\"After critical null removal: {cleaned_df.count()}\")\r\n\r\n# -----------------------------------------\r\n# Step 3: Keep Only Valid Events\r\n# -----------------------------------------\r\nvalid_events = [\"view\", \"addtocart\", \"transaction\"]\r\ncleaned_df = cleaned_df.filter(F.col(\"event\").isin(valid_events))\r\n\r\n# -----------------------------------------\r\n# Step 4: Convert Timestamp\r\n# -----------------------------------------\r\ncleaned_df = cleaned_df.withColumn(\r\n    \"event_time\",\r\n    F.from_unixtime((F.col(\"timestamp\") / 1000).cast(\"long\")).cast(TimestampType())\r\n)\r\n\r\n# -----------------------------------------\r\n# Step 5: Remove Future Dates\r\n# -----------------------------------------\r\nmax_valid_date = F.lit(datetime(2015, 9, 30)).cast(TimestampType())\r\ncleaned_df = cleaned_df.filter(F.col(\"event_time\") <= max_valid_date)\r\n\r\n# -----------------------------------------\r\n# Step 6: Deduplicate Events (Keep First per Visitor + Item + Event Type)\r\n# -----------------------------------------\r\nwindow = Window.partitionBy(\"visitorid\", \"itemid\", \"event\").orderBy(\"timestamp\")\r\ncleaned_df = cleaned_df.withColumn(\"rank\", F.row_number().over(window)) \\\r\n                       .filter(F.col(\"rank\") == 1) \\\r\n                       .drop(\"rank\")\r\n\r\n# -----------------------------------------\r\n# Step 7: Validate ID Columns\r\n# -----------------------------------------\r\ncleaned_df = cleaned_df.filter(F.col(\"visitorid\").rlike(\"^\\\\d+$\"))\r\ncleaned_df = cleaned_df.filter(F.col(\"itemid\").cast(\"int\") > 0)\r\n\r\n# -----------------------------------------\r\n# Step 8: Add Session ID\r\n# -----------------------------------------\r\ncleaned_df = cleaned_df.withColumn(\r\n    \"session_id\",\r\n    F.concat(F.col(\"visitorid\"), F.lit(\"_\"), F.date_format(\"event_time\", \"yyyyMMdd\"))\r\n)\r\n\r\n# -----------------------------------------\r\n# Step 9: Save Cleaned Data to S3 (Partitioned)\r\n# -----------------------------------------\r\n(cleaned_df\r\n .write\r\n .partitionBy(\"event\")  # Useful for optimized queries later\r\n .mode(\"overwrite\")\r\n .parquet(\"s3://ecommerce-user-behavior-2025/cleaned-data/events/\")\r\n)\r\n\r\n# -----------------------------------------\r\n# Step 10: Summary Logs\r\n# -----------------------------------------\r\nprint(f\"Final row count: {cleaned_df.count()}\")\r\n\r\nprint(\"Event distribution:\")\r\ncleaned_df.groupBy(\"event\").count().show()\r\n\r\nprint(\"Data quality metrics:\")\r\ncleaned_df.agg(\r\n    F.countDistinct(\"visitorid\").alias(\"unique_users\"),\r\n    F.min(\"event_time\").alias(\"first_event\"),\r\n    F.max(\"event_time\").alias(\"last_event\")\r\n).show()\r\n\r\n# -----------------------------------------\r\n# End Job\r\n# -----------------------------------------\r\njob.commit()\r\n"
}