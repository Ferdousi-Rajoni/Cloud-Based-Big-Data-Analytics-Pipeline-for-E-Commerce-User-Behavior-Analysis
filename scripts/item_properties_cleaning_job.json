{
	"jobConfig": {
		"name": "item_properties_cleaning_job",
		"description": "",
		"role": "arn:aws:iam::727618352148:role/AWSGlueRoleForEcommerceProject",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "item_properties_cleaning_job.py",
		"scriptLocation": "s3://ecommerce-user-behavior-2025/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-07-23T23:36:18.818Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-727618352148-ca-central-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-727618352148-ca-central-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom pyspark.sql import functions as F\r\nfrom pyspark.sql.types import TimestampType, DoubleType, BooleanType\r\nfrom pyspark.sql.window import Window\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom pyspark.context import SparkContext\r\n\r\n# Initialize Glue context\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# Step 1: Read and merge both files with defined schema\r\nschema = \"`timestamp` STRING, `itemid` STRING, `property` STRING, `value` STRING\"\r\n\r\ndf1 = spark.read.schema(schema).option(\"header\", \"true\").csv(\r\n    \"s3://ecommerce-user-behavior-2025/raw-data/retailrocket/item_properties_part1.csv\"\r\n)\r\ndf2 = spark.read.schema(schema).option(\"header\", \"true\").csv(\r\n    \"s3://ecommerce-user-behavior-2025/raw-data/retailrocket/item_properties_part2.csv\"\r\n)\r\nmerged_df = df1.unionByName(df2)\r\nprint(f\"Initial row count: {merged_df.count()}\")\r\n\r\n# Step 2: Drop nulls and clean whitespaces\r\ncleaned_df = merged_df.dropna(subset=[\"itemid\", \"property\", \"value\"])\r\ncleaned_df = cleaned_df.withColumn(\"property\", F.trim(F.col(\"property\")))\r\ncleaned_df = cleaned_df.withColumn(\"value\", F.trim(F.col(\"value\")))\r\n\r\n# Step 3: Convert timestamp\r\ncleaned_df = cleaned_df.withColumn(\r\n    \"event_time\",\r\n    F.from_unixtime(F.col(\"timestamp\").cast(\"long\") / 1000).cast(TimestampType())\r\n).drop(\"timestamp\")\r\n\r\n# Step 4: Keep only most recent value per item-property\r\nwindow = Window.partitionBy(\"itemid\", \"property\").orderBy(F.desc(\"event_time\"))\r\ncleaned_df = cleaned_df.withColumn(\"row_num\", F.row_number().over(window)) \\\r\n                       .filter(F.col(\"row_num\") == 1) \\\r\n                       .drop(\"row_num\")\r\n\r\n# Step 5: Normalize property names\r\ncleaned_df = cleaned_df.withColumn(\r\n    \"property_normalized\",\r\n    F.lower(F.regexp_replace(\"property\", \"[^a-zA-Z0-9]\", \"\"))\r\n)\r\n\r\n# Step 6: Handle numeric properties\r\nnumeric_props = [\"price\", \"cost\", \"weight\", \"length\", \"width\", \"height\"]\r\nfor prop in numeric_props:\r\n    cleaned_df = cleaned_df.withColumn(\r\n        \"value\",\r\n        F.when(F.col(\"property_normalized\") == prop,\r\n               F.regexp_replace(\"value\", \"[^0-9.]\", \"\").cast(DoubleType()))\r\n         .otherwise(F.col(\"value\"))\r\n    )\r\n\r\n\r\n\r\n# Step 8: Create wide-format key properties\r\nkey_properties = [\"categoryid\", \"price\", \"available\", \"brand\", \"color\"]\r\nwide_df = cleaned_df.filter(F.col(\"property_normalized\").isin(key_properties)) \\\r\n                    .groupBy(\"itemid\") \\\r\n                    .pivot(\"property_normalized\") \\\r\n                    .agg(F.first(\"value\"))\r\n\r\n# Step 9: Critical validation checks (optional but helpful)\r\n# Price range filtering\r\nif \"price\" in wide_df.columns:\r\n    wide_df = wide_df.filter((F.col(\"price\") > 0) & (F.col(\"price\") < 10000))\r\n\r\n# Category ID validation\r\nif \"categoryid\" in wide_df.columns:\r\n    wide_df = wide_df.filter(F.col(\"categoryid\").isNotNull())\r\n\r\n# Availability rate\r\nif \"available\" in wide_df.columns:\r\n    availability_rate = wide_df.agg(\r\n        F.mean(F.when(F.col(\"available\") == True, 1).otherwise(0)).alias(\"available_rate\")\r\n    ).collect()[0][\"available_rate\"]\r\n    print(f\"✅ Item availability rate: {availability_rate * 100:.2f}%\")\r\n\r\n# Step 10: Write outputs\r\n# Long format\r\ncleaned_df.write.mode(\"overwrite\").parquet(\r\n    \"s3://ecommerce-user-behavior-2025/cleaned-data/item_properties/long/\"\r\n)\r\n\r\n# Wide format\r\nwide_df.write.mode(\"overwrite\").parquet(\r\n    \"s3://ecommerce-user-behavior-2025/cleaned-data/item_properties/wide/\"\r\n)\r\n\r\nprint(f\"✅ Long format row count: {cleaned_df.count()}\")\r\nprint(f\"✅ Wide format row count: {wide_df.count()}\")\r\nprint(f\"✅ Unique items in wide format: {wide_df.select('itemid').distinct().count()}\")\r\n\r\n# Finalize\r\njob.commit()\r\n"
}